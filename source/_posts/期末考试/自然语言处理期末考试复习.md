---
title: 自然语言处理期末考试复习
categories: 期末考试
tags:
  - 自然语言处理
abbrlink: nlp
toc: true
date: 2023-06-24 21:13:25
sticky:
---

选择题，15道，30分
填空题，5道，10分
判断题，6道，12分
简答题，4道，20分
综合题，2道，18分
程序设计题，1道，10分

<!-- more -->

1. (单选题)以下哪个算法是无监督信息抽取算法？
A. CRF
B. SVM
C. TextRank
D. BM25
正确答案: C

2. (单选题)可以用于计算文档与查询之间的相关性得分的是：
A. CRF
B. SVM
C. TextRank
D. BM25
正确答案: D:BM25;

3. (单选题)属于信息提取的一种任务的是：
A. 文本分类
B. 机器翻译
C. 命名实体识别
D. 语言模型
正确答案: C

4. (单选题)哪个算法可以用于从文本中自动抽取实体、关系或事件等信息？
A. CRF
B. SVM
C. TextRank
D. 无监督信息抽取算法
正确答案: D

5. (单选题)属于一种基于规则的信息抽取方法的是：
A.  CRF
B. SVM
C. 基于模式匹配的方法
D. 基于聚类的方法
正确答案: C

6. (单选题)可以用于将文本中的实体和关系表示为图，从中抽取出实体之间的关系的是：
A. CRF
B. SVM
C. TextRank
D. 基于图模型的方法
正确答案: D

7. 以下哪个选项不是分离超平面的一部分
A. 特征向量
B. 阈值
С. 权重向量
D. 偏置
正确答案：A

8. 以下使用感知机的人名性别分类任务中，解释正确的是
A. 感知机算法只能用于二分类问题。
B. 人名性别分类问题是一个线性可分问题。
C. 感知机算法的损失函数是平方损失函数。
D. 感知机算法使用梯度下降来更新权重向量和偏置。
正确答案：B

## TF-IDF

TF-IDF（Term Frequency-Inverse Document Frequency）是一种常用的文本特征表示方法，用于衡量一个词对文本的重要性。

TF（Term Frequency）表示词频，指的是某个词在文本中出现的频率。如果一个词在文本中多次出现，那么它的词频较高。

IDF（Inverse Document Frequency）表示逆文档频率，用于衡量一个词的普遍重要性。在语料库中，如果一个词在多个文档中都出现，那么它的IDF较低。而如果一个词只在少数几个文档中出现，那么它的IDF较高。

TF-IDF的计算方法是将一个词的TF值与其IDF值相乘，以便得到一个词的综合重要性分数。公式如下：

TF-IDF = TF * IDF

TF-IDF的应用广泛，常用于文本挖掘、信息检索、文本分类等任务中。高TF-IDF值的词往往是该文本中重要的关键词，可以用于提取关键词、计算文本相似度、进行文本分类等。

## 切分算法

## 依存句法分析

## 线性分类模型与感知机算法

线性分类模型是一种基于线性假设的分类算法，它将输入的特征和权重进行线性组合，然后使用一个阈值函数来判断分类结果。常见的线性分类模型包括逻辑回归和线性支持向量机（SVM）等。

感知机算法是一种经典的线性二分类算法，它通过不断迭代优化权重参数来划分两个类别的数据点。感知机算法基于感知机模型，其中每个特征的权重与特征的线性组合决定了新样本的分类结果。

线性分类模型和感知机算法也有一些局限性，例如对非线性问题的处理能力有限。

## 基于双数组字典树的AC自动机

基于双数组字典树（Double-Array Trie）的AC自动机是一种用于多模式字符串匹配的高效算法，常用于自然语言处理中的文本分析和关键词匹配任务。

下面是基于双数组字典树的AC自动机的基本步骤：

1. 构建Trie树：将需要匹配的关键词构建成Trie树，每个节点表示一个字符，使用一个数组保存子节点的指针。
2. 计算Fail指针：对Trie树进行遍历，为每个节点计算Fail指针，表示在匹配失败时应跳转的下一个节点。这样可以在匹配过程中遇到不匹配的字符时快速跳转到下一个可能匹配的位置。
3. 构建双数组：将Trie树和Fail指针进行转化，得到双数组，用两个数组实现双向关联，可以在不增加额外空间的情况下高效地存储和查询。
4. 匹配过程：对于待匹配的文本，从头开始，依次匹配字符。根据当前字符和当前节点，根据转移条件找到下一个节点。如果找不到，根据Fail指针跳转到下一个可能匹配的节点。同时检查当前节点是否为一个关键词的结尾，如果是，则表示匹配到一个关键词。

基于双数组字典树的AC自动机具有高效的匹配速度和较小的内存占用，可以在大规模的关键词集合中进行快速匹配。它广泛应用于文本过滤、敏感词过滤、关键词提取等自然语言处理任务中。

以下是一个简单的基于双数组字典树的AC自动机的示例代码：

```python
# 节点类
class Node:
    def __init__(self):
        self.children = {}  # 子节点
        self.fail = None    # Fail指针
        self.is_end = False  # 是否匹配结束

# AC自动机类
class AhoCorasick:
    def __init__(self):
        self.root = Node()
    
    # 构建Trie树
    def build_trie(self, words):
        for word in words:
            node = self.root
            for c in word:
                if c not in node.children:
                    node.children[c] = Node()
                node = node.children[c]
            node.is_end = True
    
    # 计算Fail指针
    def compute_fail(self):
        queue = []
        # 根节点的Fail指针为自身
        self.root.fail = self.root
        for child in self.root.children.values():
            child.fail = self.root
            queue.append(child)
        
        # 广度优先遍历设置Fail指针
        while queue:
            curr = queue.pop(0)
            for char, child in curr.children.items():
                if curr == self.root:
                    child.fail = self.root
                else:
                    fail_node = curr.fail
                    while fail_node:
                        if char in fail_node.children:
                            child.fail = fail_node.children[char]
                            break
                        fail_node = fail_node.fail
                    else:
                        child.fail = self.root
                queue.append(child)
    
    # 匹配过程
    def match(self, text):
        result = []
        node = self.root
        for i in range(len(text)):
            char = text[i]
            while char not in node.children and node != self.root:
                node = node.fail
            if char in node.children:
                node = node.children[char]
            else:
                node = self.root

            temp = node
            while temp != self.root:
                if temp.is_end:
                    result.append((i - len(word) + 1, word))
                temp = temp.fail
        return result

# 示例代码
ac = AhoCorasick()
words = ['apple', 'banana', 'peach']
ac.build_trie(words)
ac.compute_fail()

text = "I like apple and banana"
matched = ac.match(text)

for start, word in matched:
    print(f"Matched word '{word}' at index {start}")
```

在示例代码中，首先创建了一个AC自动机对象并调用`build_trie`方法构建Trie树，然后调用`compute_fail`方法计算Fail指针。接下来，通过调用`match`方法可以对文本进行关键词匹配，并返回匹配结果。


## Arc-Eager转移系统

Arc-Eager转移系统是自然语言处理中用于依存句法分析的一种常见转移系统。它是一种基于转移动作的句法分析方法，通过一系列转移操作来构建句子的依存树结构。

Arc-Eager转移系统的核心操作包括以下几种：

1. Shift：将当前分析的词移到栈顶，将输入中的下一个词移入缓冲区。
2. Reduce：从栈顶弹出一个词，表示该词已经完成分析。
3. Left-Arc：建立一条从栈顶词指向下一个词的依存弧，然后将下一个词移入栈顶。
4. Right-Arc：建立一条从下一个词指向栈顶词的依存弧，然后从栈顶弹出词。

Arc-Eager转移系统的基本思路是通过Shift、Reduce、Left-Arc和Right-Arc这些转移动作，逐步构建句子的依存结构。在依存分析开始时，句子中的词会依次进入缓冲区，并且栈初始为空。

通过不断执行转移动作，直到分析完成，即缓冲区和栈同时为空，或者达到某个终止条件。

Arc-Eager转移系统的优点之一是能够有效地处理左依存和右依存的关系，因此在实际应用中得到广泛应用。同时，基于转移的句法分析方法还可以与其他技术（如特征模板和学习算法）结合，进一步提升依存分析的性能。

## TextRank 算法


TextRank算法是一种基于图的文本摘要和关键词提取算法，它通过分析文本中词语之间的关系来确定重要的句子或关键词。

TextRank算法的基本思想是将文本表示为一个无向加权图，其中每个节点表示一个句子或一个词语，边表示节点之间的关系。通过对图进行迭代计算，可以得到节点的重要性分数，从而确定重要的句子或关键词。

具体来说，TextRank算法包括以下步骤：

1. 分词和词性标注：对文本进行分词，并标注每个词语的词性。
2. 构建图模型：根据分词结果，构建一个无向图，其中每个节点代表一个句子或一个词语，边的权重表示节点之间的关系。通常可以使用共现关系、语义关系等来确定边的权重。
3. 迭代计算节点重要性：通过迭代计算，按照一定的更新规则，更新每个节点的重要性分数。更新规则可以基于PageRank算法进行定义，即节点的重要性由其相邻节点的重要性确定。
4. 根据节点重要性排序：根据节点的重要性分数，对句子或词语进行排序，从而确定重要的句子或关键词。

TextRank算法在自然语言处理中广泛应用于文本摘要、关键词提取、文本分类等任务。通过识别文本中重要的句子或关键词，可以快速概括文本的主题和内容，提高文本的可读性和信息提取效果。

需要注意的是，TextRank算法是一种无监督学习方法，对文本的预处理和参数的设置都会对算法的效果产生一定影响，因此在实际应用中需要根据具体任务和数据集的特点进行调整和优化。

## 维特比算法（不用纠结计算问题)

维特比算法是一种常用的动态规划算法，用于求解隐马尔可夫模型（Hidden Markov Model，HMM）中的最优路径问题。在自然语言处理中，维特比算法常被应用于词性标注、语音识别、机器翻译等任务中。

维特比算法的目标是找到一条最有可能的隐含状态序列，该序列可以解释观测序列的产生过程。下面是维特比算法的一般步骤：

1. 定义HMM模型参数：
   - 首先，需要定义隐马尔可夫模型的参数，包括状态集合、观测集合、初始状态概率、状态转移概率和观测概率。
2. 初始化：
   - 在维特比算法中，需要初始化两个矩阵：delta和psi。
   - delta矩阵表示当前时刻 t 以状态 i 结尾的最大概率，即delta[i, t]代表在时刻 t 选择状态 i 的最大概率。
   - psi矩阵记录了对应的路径，即psi[i, t]代表在时刻 t 选择状态 i 时的前一个状态。
3. 递推计算：
   - 对于每个时刻 t，计算delta[i, t]（i为状态索引）和psi[i, t]，通过下面的递推关系计算：
     - delta[i, t] = max(delta[j, t-1] * a[j, i] * b[i, o[t]])，其中j为状态索引，a为状态转移概率，b为观测概率，o为观测序列。
     - psi[i, t] = argmax(delta[j, t-1] * a[j, i])，选择使得delta[i, t]最大的前一个状态j。
4. 回溯路径：
   - 在完成所有递推计算后，可以通过psi矩阵回溯最优路径。
   - 首先，在最后一个时刻T找到具有最大delta值的状态，作为最优路径的最后一个状态。
   - 然后，根据psi矩阵从后往前回溯，通过psi[最优状态, t]找到下一个状态，依次找到整个最优路径。

维特比算法的核心思想是在动态规划的框架下，通过计算各个时刻的最大概率和对应路径，从而找到最优路径序列。这种方法减少了计算量，避免了直接枚举所有可能路径的问题。

## 齐夫定律


齐夫定律（Zipf's Law）用于描述自然语言中词频与排名的关系。

齐夫定律指出，对于自然语言中的词汇表，词频和词的排名（按照频率从高到低进行排序）之间存在着一种反比关系。具体来说，如果将词频从大到小排列，并将其对应的排名从1开始标注，那么排名为n的词的出现频次约等于1/n，或者说频次和排名之间大致满足一个指数函数关系。

齐夫定律在自然语言处理中有广泛应用。例如，在文本处理任务中，可以根据齐夫定律对词频进行统计和分析，识别出高频词、低频词以及词频分布的稀疏性等特征。这对于词汇表的特征选择、文本分类、信息检索等任务都具有一定的指导意义。

需要注意的是，齐夫定律是一种经验规律，并不适用于所有的自然语言文本。在某些特殊情况下，词频与排名之间可能存在一定的偏离。因此，在具体应用中，需要根据具体问题和数据集的特点进行分析和调整。

## Word2vec主要包含两个浅层的神经网络模型，分别是？

Word2vec主要包含两个浅层的神经网络模型，分别是：

1. CBOW（Continuous Bag-of-Words）模型：
   - CBOW模型的目标是根据上下文窗口中的周围单词来预测当前单词。
   - 在模型训练时，CBOW首先将上下文中的单词进行one-hot编码，然后通过一个隐藏层将编码输入进行线性变换和非线性变换，最后通过一个softmax层进行预测。
   - 通过最小化预测单词和实际单词之间的损失函数，CBOW模型的隐含表示学习到了单词的分布式向量表示。
2. Skip-gram模型：
   - Skip-gram模型则是相反的，它的目标是根据当前单词预测周围上下文单词。
   - 在模型训练时，Skip-gram模型也将当前单词进行one-hot编码，然后通过隐藏层进行线性变换和非线性变换，最后通过softmax层进行预测。
   - 与CBOW相比，Skip-gram模型更适合处理小规模的数据集，并且能够更好地捕捉罕见单词的上下文信息。

这两个模型的思路都是通过神经网络来训练单词的分布式表示，即word embeddings。这些分布式表示可以被用于计算单词之间的相似性、进行词向量的运算、以及作为其他自然语言处理任务的特征表示等。 Word2vec的模型结构简单而高效，成为了学习词向量的重要工具之一。

## 最常见的句法分析任务？

最常见的句法分析任务是依存句法分析（Dependency Parsing）。依存句法分析旨在分析句子的语法结构，并表示单词之间的依存关系。它通过构建一个依存关系树（dependency tree）来形式化句子的结构，其中每个单词作为一个节点，而依存关系则由边连接起来，表示单词之间的依存关系。

在依存句法分析中，通常需要确定每个单词的依存头（head）以及头与子节点之间的依存关系标签。依存头指的是某个单词在句法分析中依赖的单词，而依存关系标签则描述了头与子节点之间的语法关系，如主谓关系、定中关系、状语关系等。

依存句法分析在自然语言处理中有广泛的应用，包括句法解析、语法错误检测、机器翻译、问答系统等。通过深入理解句子的语法结构，依存句法分析有助于提取句子中的信息，更准确地理解和处理自然语言文本。

## AC自动机

AC自动机（Aho-Corasick Automaton）是一种高效的多模式串匹配算法，在自然语言处理中常用于快速查找文本中的多个关键词或模式串。

AC自动机的主要思想是将需要匹配的模式串构建成一个有限状态自动机。该自动机由多个状态和状态之间的转移边组成，其中每个状态代表当前已匹配的模式串的前缀。构建AC自动机的过程涉及两个主要步骤：

1. 构建Trie树：根据模式串集合构建一个Trie树（字典树），其中每个节点代表一个字符，路径代表字符之间的连接。Trie树的结构允许在文本中进行高效的模式匹配。
2. 添加转移边：根据Trie树的结构，为每个节点添加转移边。转移边指示了在当前状态下，如果匹配下一个字符，则应转移到哪个状态。

在匹配过程中，AC自动机根据输入文本逐个字符进行状态转移。当达到某个状态时，表示已匹配到一个模式串的前缀。同时，可以通过在每个状态上存储额外的信息，如模式串的编号、出现的位置等。

相比于传统串匹配算法，AC自动机的优势在于避免了大量的重复比较和回溯操作，从而提高了匹配的效率。在自然语言处理中，AC自动机可以应用于分词、实体识别、敏感词过滤等任务中，实现对大规模模式串的高效匹配。

## K-means 算法

K-means算法是一种常用的聚类算法，用于将数据点划分成K个不同的簇群。在自然语言处理中，K-means算法可以用于文本聚类、文本分类等任务。

K-means算法的基本思想是通过迭代优化的方式，将数据点划分到K个簇中，使得同一簇内的数据点相似度较高，不同簇之间的数据点相似度较低。算法的步骤如下：

1. 随机选择K个初始簇中心（可以是随机选取或根据启发式方法选择）。
2. 对每个数据点，计算其到各个簇中心的距离，并将数据点分配给距离最近的簇。
3. 根据当前分配的簇，更新每个簇的中心点，将簇中所有数据点的均值作为新的中心点。
4. 重复步骤2和步骤3，直到达到停止条件（如达到最大迭代次数或中心点的变化小于设定阈值）。

对于自然语言处理中的文本数据，通常需要进行向量表示，如使用词向量（Word Embedding）表示文本，然后基于这些向量进行K-means聚类。通过K-means算法，可以将相似的文本划分为同一簇，从而实现文本的自动聚类和分类。

需要注意的是，K-means算法对初始中心点的选择较为敏感，不同的初始点可能导致不同的聚类结果。同时，K-means算法也有一些限制，如对离群点敏感，对簇形状的约束较强等。因此，在使用K-means算法时，需要综合考虑数据特点和实际任务需求，选择合适的K值和初始点，适当调整算法参数，以获得较好的聚类效果。

## 隐马尔可夫模型存在哪些问题，以及算法流程

以下是HMM存在的问题：

1. 信息损失：HMM假设当前状态只依赖于前一个状态，而忽略了其他相关信息。这可能导致对序列中的某些重要特征的丢失。
2. 长期依赖：HMM不能很好地处理长期依赖，即序列中当前状态与较远的过去状态相关联的情况。
3. 参数估计：在HMM中，参数的估计有时很困难。尤其是当模型中的状态和观测空间较大时，参数估计变得更加复杂。
4. 模型选择：选择适当的状态数和观测空间大小也是HMM的一个挑战。选择大小不当可能导致模型不准确或过于复杂。

下面是HMM的算法流程：

1. 初始化：确定初始状态概率向量和状态转移概率矩阵。
2. 前向算法：根据观测序列和模型参数，计算在每个时刻t处于各个状态的概率，即前向概率。
3. 后向算法：根据观测序列和模型参数，计算在每个时刻t从各个状态转移到终止状态的概率，即后向概率。
4. E步：使用前向概率和后向概率，计算在每个时刻t处于各个状态的概率，即在给定参数下在t时刻t处于某个状态的概率。
5. M步：根据观测序列和在E步计算得到的状态概率，更新模型参数，即初始化、状态转移概率和观测概率。
6. 重复步骤2至步骤5，直到收敛或满足终止条件。

## 词向量

自然语言处理中的词向量是一种表示单词的数学表示方法，可以将单词转换为连续向量空间中的向量。这种表示方式被广泛应用于各种自然语言处理任务中，如文本分类、情感分析、机器翻译等。

词向量的目标是捕捉单词之间的语义和语法关系，使得具有相似含义的单词在向量空间中的距离更近。这样，我们可以通过计算向量之间的距离或相似度来判断单词之间的关系和相似程度。

常见的词向量表示方法有以下几种：

1. One-hot向量：
   - 将每个单词表示为一个长度与词汇表大小相等的向量，只有对应单词位置的值为1，其余位置为0。
   - One-hot向量没有捕捉到单词之间的语义关系，只是简单地表示单词的存在与否。
2. 基于计数的词向量：
   - 使用统计方法（如TF-IDF，词频等）将单词映射到向量空间。
   - 通过计算单词在文本中的计数和在语料库中的计数，得到稀疏向量表示。但这种方法也没有考虑到单词之间的具体语义关系。
3. Word2Vec：
   - Word2Vec是一种通过训练神经网络模型来生成词向量的方法。它有两种主要的实现方式：CBOW（Continuous Bag-Of-Words）和Skip-gram。
   - CBOW模型根据周围的上下文单词预测目标单词，而Skip-gram模型则根据目标单词来预测周围的上下文单词。
   - Word2Vec利用神经网络的训练过程，将单词映射到连续向量空间，并且在向量空间中保留了一定的语义信息。
4. GloVe（Global Vectors）：
   - GloVe也是一种基于统计信息的词向量表示方法，它结合了全局统计信息和局部上下文信息。
   - GloVe使用全局矩阵统计词之间的共现出现次数，并通过训练模型来生成词向量。它在一定程度上解决了Word2Vec在处理罕见词汇和频率低词汇上的问题。

这些是自然语言处理中常用的词向量表示方法，每种方法都有其优缺点和适应场景。选择适合任务需求的词向量表示可以提高模型性能和准确性。

## 互信息的计算问题

> 详情见书本 P 273

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo>;</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo data-mjx-texclass="OP">∑</mo><mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><msub><mi>log</mi><mn>2</mn></msub><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></math>

如果两者的联合分布只有一个取值则不用计算期望

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>I</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>log</mi><mn>2</mn></msub><mo data-mjx-texclass="NONE">⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac></math>


## 二元语法模型下的概率计算

假设我们有一个简单的训练语料库，其中包含以下句子：

1. I like to play football.
2. I like to eat pizza.
3. I like to watch movies.
4. I like ice cream.

现在，我们希望计算下面句子的概率： "I like to eat ice cream."

首先，我们需要统计每个词项的出现次数，并计算概率 P(w)。

| 词项     | 出现次数 | 概率 |
| -------- | -------- | ---- |
| I        | 4        | 4/16 |
| like     | 4        | 4/16 |
| to       | 4        | 4/16 |
| play     | 1        | 1/16 |
| football | 1        | 1/16 |
| eat      | 1        | 1/16 |
| pizza    | 1        | 1/16 |
| watch    | 1        | 1/16 |
| movies   | 1        | 1/16 |
| ice      | 1        | 1/16 |
| cream    | 1        | 1/16 |

接下来，我们统计每个相邻词对的出现次数，并计算条件概率 P(wi | wi-1)。

| 相邻词对         | 出现次数 | 条件概率 |
| ---------------- | -------- | -------- |
| (I, like)        | 4        | 4/4      |
| (like, to)       | 4        | 4/4      |
| (to, play)       | 1        | 1/4      |
| (play, football) | 1        | 1/1      |
| (to, eat)        | 1        | 1/4      |
| (eat, pizza)     | 1        | 1/1      |
| (to, watch)      | 1        | 1/4      |
| (watch, movies)  | 1        | 1/1      |
| (I, like)        | 4        | 4/4      |
| (like, ice)      | 1        | 1/4      |
| (ice, cream)     | 1        | 1/1      |

根据这些统计结果，我们可以计算给定句子 "I like to eat ice cream." 的概率。

P("I like to eat ice cream.") = P(I) * P(like | I) * P(to | like) * P(eat | to) * P(ice | eat) * P(cream | ice)

= (4/16) * (4/4) * (4/4) * (1/4) * (1/4) * (1/1)

= 1/64

因此，"I like to eat ice cream." 在二元语法模型下的概率为 1/64。